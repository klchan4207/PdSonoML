{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this .ipynb file is modified from _optuna_VALI.py for demonstration\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import dgl\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import datetime , time\n",
    "import json\n",
    "import copy\n",
    "import math\n",
    "import argparse\n",
    "import glob\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    __file__\n",
    "except:\n",
    "    __file__ = os.path.join(os.getcwd(),'_optuna_VALI.ipynb')\n",
    "\n",
    "\n",
    "_DEMO = True # added for this folder only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "currentdir = os.path.dirname(os.path.realpath(__file__))\n",
    "sys.path.insert(1, currentdir+\"/../../Model\")\n",
    "sys.path.insert(1, currentdir+\"/../..\")\n",
    "_func = __import__(\"_func\")\n",
    "\n",
    "#________________________________________________________________________________________________________\n",
    "\n",
    "# get cpu/gpu\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "_detect_gpus = tf.config.list_logical_devices('GPU')\n",
    "if len(_detect_gpus)==0:\n",
    "    _device_type=\"cpu\"\n",
    "else:\n",
    "    _device_type=\"gpu\"\n",
    "\n",
    "\n",
    "#________________________________________________________________________________________________________\n",
    "\n",
    "# Helper function to get data\n",
    "get_data =  _func.get_data\n",
    "\n",
    "# model defining\n",
    "def create_model(trial_setup_dict): # normalizer always = None\n",
    "    print(\"Model ...\")\n",
    "        \n",
    "    GNN_model = __import__(trial_setup_dict['modeltype_pyfilename'])\n",
    "\n",
    "    model = GNN_model.MODEL(trial_setup_dict['model'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_optimizer(trial_setup_dict):\n",
    "    print(\"Optimizer ...\")\n",
    "    optimizer_chosen = trial_setup_dict[\"optimizer_chosen\"]\n",
    "    kwargs = trial_setup_dict[optimizer_chosen]\n",
    "\n",
    "    optimizer = getattr(tf.optimizers, optimizer_chosen)(**kwargs)\n",
    "    return optimizer\n",
    "\n",
    "def learn(model, \n",
    "            optimizer, \n",
    "            train_dataset, \n",
    "            loss_mode,\n",
    "            mode=\"eval\", \n",
    "            feature_type=\"normal\" , \n",
    "            training_seed = None, \n",
    "            _device_id=\"0\"):\n",
    "    MAE = tf.keras.losses.MeanAbsoluteError()\n",
    "    MAPE = tf.keras.losses.MeanAbsolutePercentageError()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "\n",
    "    if mode == \"train\" and training_seed == None:\n",
    "        print(\"ERROR, require to set a training seed\")\n",
    "        exit()\n",
    "\n",
    "    # turn off\n",
    "    # expect the input to be sth like _1\n",
    "    #_device_id=_device_id.replace(\"_\",\"\")\n",
    "    #if _device_id==\"\":\n",
    "    #    _device_id = \"0\"\n",
    "    _device_id = \"0\"\n",
    "    _device = \"/{0}:{1}\".format(_device_type,_device_id)\n",
    "    with tf.device(_device):\n",
    "        for batch, (features, labels) in enumerate(train_dataset):\n",
    "            labels = tf.convert_to_tensor(labels)\n",
    "            with tf.GradientTape() as tape:\n",
    "                # set seed for reproducibility\n",
    "                if mode == \"train\":\n",
    "                    tf.random.set_seed(training_seed)\n",
    "                logits = model(features, training=(mode==\"train\"), _device=_device)\n",
    "                if loss_mode == 'MAE':\n",
    "                    loss = MAE(logits,labels)\n",
    "                elif loss_mode == 'MAPE':\n",
    "                    loss = MAPE(labels,logits)\n",
    "                \n",
    "            if feature_type == \"normal\":\n",
    "                current_batch_size = len(features)\n",
    "            else:\n",
    "                current_batch_size = len(features[0])\n",
    "\n",
    "            total += current_batch_size\n",
    "            epoch_loss = (epoch_loss*(total-current_batch_size) + loss*current_batch_size)/total\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            if (mode==\"train\"):\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "\n",
    "    if mode == \"eval\":\n",
    "        print(\"Eval ...\")\n",
    "    elif mode == \"eval_silent\":\n",
    "        pass\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, optimizer, test_dataset,trial_setup_dict):\n",
    "\n",
    "    \n",
    "    method = trial_setup_dict[\"eval_method\"]\n",
    "\n",
    "    if method == \"default\":\n",
    "        loss_mode=trial_setup_dict['loss_mode']\n",
    "        return learn(model, optimizer, test_dataset, loss_mode, \"eval_silent\", feature_type = 'split' )\n",
    "\n",
    "    elif method == \"r_square\":\n",
    "        # pretreatment\n",
    "        test_features = np.concatenate([x for _, (x, _) in enumerate(test_dataset)])\n",
    "        test_values = np.concatenate([y for _, (_, y) in enumerate(test_dataset)])\n",
    "        pred = model.predict(np.array(test_features))\n",
    "        real = test_values\n",
    "        _pred = np.array(pred).reshape(-1, 1)\n",
    "        _real = np.array(real)\n",
    "        linreg = LinearRegression(normalize=False,fit_intercept=True).fit(_pred,_real)\n",
    "        linreg.coef_ = np.array([1])\n",
    "        linreg.intercept_ = 0 \n",
    "        get_r_square = linreg.score(_pred, _real)\n",
    "        return get_r_square\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial,optuna_setup_dict, mode=\"trial\"):\n",
    "    print(\"Objective ...\")\n",
    "\n",
    "    if mode==\"trial\":\n",
    "        #initialize trial\n",
    "        for k,v in optuna_setup_dict.items():\n",
    "            # treat numbers with range\n",
    "            if type(v)==dict:\n",
    "                v_type = list(v.keys())[0]\n",
    "                if v_type == 'categorical':\n",
    "                    v_choices = v[v_type]\n",
    "                    #if type(v_choices) == list:\n",
    "                    trial.suggest_categorical(k,v_choices )\n",
    "                    #else:\n",
    "                    #    print(\"##### syntax ERROR #####, at:\",k,v)\n",
    "                elif v_type == 'discrete_uniform':\n",
    "                    v_low = v[v_type][0]\n",
    "                    v_high = v[v_type][1]\n",
    "                    v_q = v[v_type][2]  \n",
    "                    trial.suggest_discrete_uniform(k, v_low,v_high,v_q)\n",
    "                elif v_type in ['float','int']:\n",
    "                    v_low = v[v_type][0]\n",
    "                    v_high = v[v_type][1]\n",
    "                    if v_type == 'float': v_step = None\n",
    "                    elif v_type == 'int': v_step = 1\n",
    "                    v_log = False\n",
    "                    if len(v[v_type]) > 2:\n",
    "                        v_optionals = v[v_type][2]\n",
    "                        v_step = v_optionals.get('step')\n",
    "                        v_log = v_optionals.get('log')\n",
    "                        if v_log == None:\n",
    "                            v_log = False\n",
    "                    if v_step and v_log:\n",
    "                        print(\"ERROR at {}, step and log cannot be used together:\".foramt(v_type),k,v)\n",
    "                        exit()\n",
    "                    else:\n",
    "                        if v_type == 'float':\n",
    "                            trial.suggest_float(k, v_low, v_high, step=v_step,log=v_log)\n",
    "                        elif v_type == 'int':\n",
    "                            trial.suggest_int(k, v_low, v_high, step=v_step,log=v_log)\n",
    "                elif v_type in ['loguniform','uniform']:\n",
    "                    v_low = v[v_type][0]\n",
    "                    v_high = v[v_type][1]\n",
    "                    if v_type == 'loguniform':\n",
    "                        trial.suggest_loguniform(k, v_low, v_high)\n",
    "                    elif v_type == 'uniform':\n",
    "                        trial.suggest_uniform(k, v_low, v_high)\n",
    "                else:\n",
    "                    print(\"ERROR v_type {}\",k,v)\n",
    "                print(\"setting {} to param {} \".format(v,k))\n",
    "            else:\n",
    "                print(\"setting {} to attr {} \".format(v,k))\n",
    "                trial.set_user_attr(k, v)\n",
    "\n",
    "\n",
    "        def merge(d1, d2):\n",
    "            for k in d2:\n",
    "                if k in d1 and isinstance(d1[k], dict) and isinstance(d2[k], dict):\n",
    "                    merge(d1[k], d2[k])\n",
    "                else:\n",
    "                    d1[k] = d2[k]\n",
    "        def copy_merge(d1,d2):\n",
    "            _d1 = copy.deepcopy(d1)\n",
    "            _d2 = copy.deepcopy(d2)\n",
    "            merge(_d1,_d2)\n",
    "            return _d1\n",
    "\n",
    "        trial_setup_dict = {}\n",
    "        for _dict in [trial.user_attrs , trial.params]:\n",
    "            for k,v in _dict.items():\n",
    "                if \"|\" in k:\n",
    "                    _kname_list = k.split(\"|\")\n",
    "                    _kname_list.reverse()\n",
    "                    _dict = v\n",
    "                    for _kname in _kname_list:\n",
    "                        _dict = {_kname:_dict }\n",
    "                    trial_setup_dict = copy_merge(trial_setup_dict,_dict)\n",
    "                else:\n",
    "                    trial_setup_dict[k] = v\n",
    "    elif mode==\"simple_train\" or  mode==\"cont_train\":\n",
    "        trial_setup_dict =  copy.deepcopy(optuna_setup_dict)\n",
    "\n",
    "\n",
    "    # choose optuna mode\n",
    "    opttrain_mode = trial_setup_dict['opttrain_mode']\n",
    "    #opttrain = __import__(opttrain_mode)\n",
    "\n",
    "    # refine trial inputs\n",
    "    trial_setup_dict['input_ndata_dim'] = len(trial_setup_dict['input_ndata_list'])\n",
    "    trial_setup_dict['input_edata_dim'] = len(trial_setup_dict['input_edata_list'])\n",
    "    devid_suffix = trial_setup_dict['devid_suffix']\n",
    "\n",
    "    #________________________________________________________________________________________________________\n",
    "    # define saving directory\n",
    "    z = datetime.datetime.now()\n",
    "    study_foldername = trial_setup_dict['study_foldername']\n",
    "    if mode != \"cont_train\": #trial_setup_dict.get('modeltrained_foldername') == None:\n",
    "        if _DEMO:\n",
    "            modeltrained_foldername = \"_{0}_DEMO_s{1:02d}_ms{2:06d}_model\".format(study_foldername,z.second,z.microsecond).replace(\"-\",\"\")\n",
    "        else:\n",
    "            pass\n",
    "            #modeltrained_foldername = \"_{0}_{1}_{2:02d}_{3:02d}_s{4:02d}_ms{5:06d}_model\".format(study_foldername,z.date(),z.hour,z.minute,z.second,z.microsecond).replace(\"-\",\"\")\n",
    "        trial_setup_dict['modeltrained_foldername'] = modeltrained_foldername\n",
    "    else:\n",
    "        loss_mode = trial_setup_dict['loss_mode']\n",
    "        modeltrained_foldername_prev = trial_setup_dict['modeltrained_foldername']\n",
    "        trial_setup_dict['modeltrained_foldername'] = modeltrained_foldername_prev+\"_##{}_CONT\".format(loss_mode)\n",
    "        modeltrained_foldername = trial_setup_dict['modeltrained_foldername']\n",
    "    saving_dir = currentdir+\"/\"+modeltrained_foldername\n",
    "\n",
    "\n",
    "    stdoutOrigin=sys.stdout \n",
    "    if trial_setup_dict.get('_test_mode') == \"1\":\n",
    "        log_filename = \"_TEST_trainlog\"\n",
    "    else:\n",
    "        log_filename = modeltrained_foldername+\"_trainlog\"\n",
    "\n",
    "    sys.stdout = open(log_filename, \"w\")\n",
    "\n",
    "    #________________________________________________________________________________________________________\n",
    "    # gen seed if not specified\n",
    "    if trial_setup_dict.get('training_seed') == None:\n",
    "        trial_setup_dict['training_seed'] = random.randint(0,9999)\n",
    "    training_seed = trial_setup_dict['training_seed']\n",
    "\n",
    "    print('trial_setup_dict')\n",
    "    print(json.dumps(trial_setup_dict,indent=4))\n",
    "\n",
    "    print('training_seed=',training_seed)\n",
    "\n",
    "\n",
    "    #________________________________________________________________________________________________________\n",
    "    trial_begin = time.time()\n",
    "    # Get train/test data.\n",
    "    _output_data_dict = _func.get_data(\n",
    "        trial_setup_dict,\n",
    "        batch_size=trial_setup_dict['batch_size'],\n",
    "        test_split=trial_setup_dict['test_split'],\n",
    "        vali_split=trial_setup_dict['vali_split'],\n",
    "        currentdir = currentdir,\n",
    "        return_indexes=True\n",
    "    )\n",
    "    dummy_train_dataset         = _output_data_dict['stage1_train_dataset']\n",
    "    train_dataset               = _output_data_dict['train_dataset']\n",
    "    dummy_vali_dataset          = _output_data_dict['stage1_vali_dataset']\n",
    "    vali_dataset                = _output_data_dict['vali_dataset']\n",
    "    dummy_test_dataset          = _output_data_dict['stage1_test_dataset']\n",
    "    test_dataset                = _output_data_dict['test_dataset']\n",
    "\n",
    "    node_num_info               = _output_data_dict['node_num_info']\n",
    "    train_vali_test_indexes     = _output_data_dict['train_vali_test_indexes']\n",
    "\n",
    "    if trial_setup_dict.get('node_num_info') == None:\n",
    "        trial_setup_dict['node_num_info'] = node_num_info\n",
    "\n",
    "    # Build model and optimizer.\n",
    "    model = create_model(trial_setup_dict)\n",
    "    optimizer = create_optimizer(trial_setup_dict)\n",
    "\n",
    "    # setup for saving models\n",
    "    if mode==\"cont_train\":\n",
    "        print(\"...Restoring checkpoint\")\n",
    "        # restore ckpt\n",
    "        ckpt = tf.train.Checkpoint(\n",
    "            step=tf.Variable(1), optimizer=optimizer, net=model\n",
    "        )\n",
    "        saving_dir_prev = currentdir+\"/{}/{}\".format(modeltrained_foldername_prev,modeltrained_foldername_prev)\n",
    "        _manager = tf.train.CheckpointManager(ckpt, saving_dir_prev, max_to_keep=3)\n",
    "        ckpt.restore(_manager.latest_checkpoint)\n",
    "        if _manager.latest_checkpoint:\n",
    "            print(\"Restored from {}\".format(_manager.latest_checkpoint))\n",
    "    else:\n",
    "        # create ckpt\n",
    "        ckpt = tf.train.Checkpoint(\n",
    "            step=tf.Variable(1), optimizer=optimizer, net=model\n",
    "        )\n",
    "    manager = tf.train.CheckpointManager(ckpt, saving_dir+\"/{}\".format(saving_dir.split(\"/\")[-1]\n",
    "    ) \n",
    "        , max_to_keep=3)\n",
    "\n",
    "    # gen cutoff\n",
    "    stage1_cutoff = trial_setup_dict['stage1_cutoff']\n",
    "    stage1_skip = trial_setup_dict.get('stage1_skip')\n",
    "    stage2_converge = trial_setup_dict.get('stage2_converge')\n",
    "\n",
    "    stage2_fluctuate_dv = trial_setup_dict['stage2_fluctuate_dv']\n",
    "    stage2_fluctuate_pc = trial_setup_dict['stage2_fluctuate_pc']\n",
    "    if stage2_converge == None:\n",
    "        stage2_converge = 0.0\n",
    "    loss_mode = trial_setup_dict['loss_mode']\n",
    "\n",
    "    loss_record_dict = {1:{},2:{}}\n",
    "    # itterate training sequence:\n",
    "    for _stage in [1,2]:\n",
    "        max_epoch_num = trial_setup_dict['max_epoch_num']\n",
    "\n",
    "        if _stage==1:\n",
    "            print(\"################## STAGE  1 #######################\")\n",
    "            model.stopuseArrheniusEq()\n",
    "            trial_train_dataset = dummy_train_dataset\n",
    "            trial_vali_dataset = dummy_vali_dataset\n",
    "            trial_test_dataset = dummy_test_dataset\n",
    "            if stage1_skip:\n",
    "                print(\"Skipping stage 1\")\n",
    "                continue\n",
    "        if _stage==2:\n",
    "            print(\"################## STAGE  2 #######################\")\n",
    "            model.startuseArrheniusEq()\n",
    "            trial_train_dataset = train_dataset\n",
    "            trial_vali_dataset = vali_dataset\n",
    "            trial_test_dataset = test_dataset\n",
    "        print(\"use Arrhenius:\",model.useArrheniusEq)\n",
    "\n",
    "\n",
    "        print(\"Epoch ( [train|valid|test] ): \", end =\" \", flush=True)\n",
    "        # Training and testing cycle.\n",
    "        last10_loss = np.array([i*10 for i in range(1,11)]) # [10,... 100]\n",
    "        loss_record = {\"loss\":[],\"vali_loss\":[],\"test_loss\":[]}\n",
    "\n",
    "        for i in range(max_epoch_num):\n",
    "            loss = learn(model, optimizer, trial_train_dataset, loss_mode, \"train\", feature_type = 'split' , training_seed = training_seed) #, _device_id=devid_suffix)\n",
    "            loss_record['loss'].append(round(float(loss),7))\n",
    "            \n",
    "            vali_loss = learn(model, optimizer, trial_vali_dataset, loss_mode, \"eval_silent\", feature_type = 'split'\n",
    "            )\n",
    "            loss_record['vali_loss'].append(round(float(vali_loss),7))\n",
    "\n",
    "            test_loss = learn(model, optimizer, trial_test_dataset, loss_mode, \"eval_silent\", feature_type = 'split'\n",
    "            )\n",
    "            loss_record['test_loss'].append(round(float(test_loss),7))\n",
    "\n",
    "    \n",
    "            ckpt.step.assign_add(1)\n",
    "\n",
    "            if _stage==1 and round(float(loss),7) <=stage1_cutoff:\n",
    "                break\n",
    "\n",
    "            if _stage==2:\n",
    "                last10_loss_prev = last10_loss.copy()\n",
    "                last10_loss = last10_loss[1:]\n",
    "                last10_loss = np.append(last10_loss,[loss])\n",
    "\n",
    "                delta_loss = [last10_loss[_i]-last10_loss_prev[_i] for _i in range(len(last10_loss_prev))]\n",
    "                fluctuate_pc = sum([  0 > delta_loss[_i]*delta_loss[_i+1] for _i in range(len(delta_loss)-1)]) / (len(delta_loss)-1)\n",
    "                avg_dv = np.mean(np.abs(delta_loss))\n",
    "                \n",
    "                # break if converged\n",
    "                if avg_dv <= stage2_converge:\n",
    "                    print(\"...converged\")\n",
    "                    break\n",
    "                # break if fluctuate too much\n",
    "                if fluctuate_pc >= stage2_fluctuate_pc and avg_dv >= stage2_fluctuate_dv:\n",
    "                    print(\"...fluctuation too high:\")\n",
    "                    print(\"fluctuate_pc:\",fluctuate_pc)\n",
    "                    print(\"stage2_fluctuate_dv:\",avg_dv)\n",
    "                    break\n",
    "\n",
    "            if i%10 == 0:\n",
    "                print(\"{} [{}|{}|{}]-> \".format(i,round(float(loss),7) , round(float(vali_loss),7), round(float(test_loss),7) ), end =\" \", flush=True)\n",
    "\n",
    "        print(\"{} [{}|{}|{}]-> \".format(i,round(float(loss),7) , round(float(vali_loss),7), round(float(test_loss),7) ), end =\" \", flush=True)\n",
    "        print(\"END\")\n",
    "        if _stage==2:\n",
    "            print(\"avg_dv out of {0}: {1}\".format(len(last10_loss),avg_dv))\n",
    "        loss_record_dict[_stage] = loss_record\n",
    "\n",
    "    eval_result = evaluate(model, optimizer, test_dataset, trial_setup_dict)\n",
    "    trial_end = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    # save model\n",
    "    manager.save()\n",
    "    with open(saving_dir+\"/{}_setting\".format(saving_dir.split(\"/\")[-1]\n",
    "    ),\"w+\") as f:\n",
    "        f.write(json.dumps(trial_setup_dict,indent=4).replace(\"\\'\",\"\\\"\"))\n",
    "    with open(saving_dir+\"/{}_train_vali_test_indexes\".format(saving_dir.split(\"/\")[-1]\n",
    "    ),\"w+\") as f:\n",
    "        f.write(json.dumps(train_vali_test_indexes,indent=4).replace(\"\\'\",\"\\\"\"))\n",
    "    with open(saving_dir+\"/loss_record\",\"w+\") as f:\n",
    "        f.write(json.dumps(loss_record_dict,indent=4).replace(\"\\'\",\"\\\"\"))\n",
    "\n",
    "\n",
    "    # save other files\n",
    "    with open(__file__,\"r\") as input:\n",
    "        raw_pyfile_record = input.read()\n",
    "\n",
    "    with open(saving_dir+\"/rawpyrecord\",\"w+\") as f:\n",
    "        f.write(raw_pyfile_record)\n",
    "        \n",
    "    print(\"This trial used {} seconds\".format(trial_end-trial_begin))\n",
    "    \n",
    "    # save logfiles\n",
    "    sys.stdout.close()\n",
    "    sys.stdout=stdoutOrigin\n",
    "    with open(log_filename, \"r\") as input:\n",
    "        _log = input.read()\n",
    "    with open(saving_dir+\"/trainlog\", \"w+\") as output:\n",
    "        output.write(_log)\n",
    "    os.remove(log_filename)\n",
    "\n",
    "    return eval_result\n",
    "\n",
    "\n",
    "def search(optuna_setup_dict,study):\n",
    "\n",
    "    print(f\"Sampler used is {study.sampler.__class__.__name__}\")\n",
    "\n",
    "    f = lambda y: objective(y,optuna_setup_dict)\n",
    "\n",
    "    study_filename = optuna_setup_dict[\"study_foldername\"]\n",
    "    study_dir = os.path.join(currentdir,study_filename,\"study\")\n",
    "\n",
    "    total_n_trials = optuna_setup_dict[\"study_total_n_trials\"]-len(study.trials)\n",
    "    num_per_batch =  optuna_setup_dict[\"study_num_per_batch\"]\n",
    "    total_batch_count = math.ceil(total_n_trials/num_per_batch)\n",
    "\n",
    "    record_study_log = optuna_setup_dict[\"study_log_input\"]\n",
    "    if record_study_log:\n",
    "        study_log_inputdir = os.path.join(currentdir,optuna_setup_dict[\"study_log_input\"])\n",
    "        study_log_outputdir = os.path.join(currentdir,optuna_setup_dict[\"study_log_output\"])\n",
    "    \n",
    "    for _i in range(1,total_batch_count+1):\n",
    "\n",
    "        n_trials = 3\n",
    "        if _i*num_per_batch > total_n_trials :\n",
    "            n_trials = total_n_trials%num_per_batch\n",
    "\n",
    "        if os.path.isfile(study_dir):\n",
    "            with open(study_dir, 'rb') as _study_input:\n",
    "                study = pickle.load(_study_input)\n",
    "\n",
    "        study.optimize(f, n_trials=n_trials)\n",
    "\n",
    "        # save study\n",
    "        pickle.dump(study, open(study_dir, \"wb\"))\n",
    "\n",
    "        print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "        print(\"Best trial:\")\n",
    "        trial = study.best_trial\n",
    "\n",
    "        print(\"  Value: \", trial.value)\n",
    "\n",
    "        print(\"  Params: \")\n",
    "        for key, value in trial.params.items():\n",
    "            print(\"    {}: {}\".format(key, value)) \n",
    "\n",
    "        if record_study_log:\n",
    "            with open(study_log_inputdir,\"r\") as _study_log_input:\n",
    "                study_log = _study_log_input.read()\n",
    "            with open(study_log_outputdir,\"w+\") as _study_log_output:\n",
    "                _study_log_output.write(study_log)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_oos_ligid = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_DEMO.inp\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "_input_filename = '_DEMO.inp'\n",
    "_input_json='''{\n",
    "    \"study_total_n_trials\":1,\n",
    "    \"study_num_per_batch\":1,\n",
    "    \"study_direction\":\"minimize\",\n",
    "    \"loss_mode\":\"MAE\",\n",
    "    \"eval_method\":\"default\",\n",
    "\n",
    "    \"modeltype_pyfilename\":\"GNN\",\n",
    "    \"opttrain_mode\":\"formula_GPU\",\n",
    "    \"dataset_edge(LIG)_filename\":\"_graphBiEDGE2DATA(LIG).csv\",\n",
    "    \"dataset_node(LIG)_filename\":\"_graphNODEDATA(LIG).csv\",\n",
    "    \"dataset(RCT)_filename\":\"_rawDATA(RCT).xlsx\",\n",
    "    \"dataset(RCTxLIG)_filename\":\"_rawDATA(RCTxLIG)_deltaE.xlsx\",\n",
    "    \"constant_node_num\":1,\n",
    "    \"input_ndata_list\":[\"boolean\"],\n",
    "    \"input_edata_list\":[\"weight\"],\n",
    "\n",
    "    \"test_split\":0.2,\n",
    "    \"vali_split\":0.2,\n",
    "\n",
    "    \"stage1_cutoff\":{\n",
    "\t\t\"categorical\":[5.0,6.0,7.0]\n",
    "\t\t},\n",
    "    \"stage2_converge\":1e-5,\n",
    "    \"stage2_fluctuate_pc\":0.8,\n",
    "    \"stage2_fluctuate_dv\":0.2,\n",
    "    \"optimizer_chosen\":\"Adam\",\n",
    "    \"Adam|learning_rate\":{\n",
    "\t\t\"categorical\":[0.0005,0.001,0.002,0.005]\n",
    "\t\t},\n",
    "    \"max_epoch_num\":100,\n",
    "    \"batch_size\":10,\n",
    "\n",
    "\n",
    "\n",
    "    \"model|MPNN|message_dim\":{\n",
    "\t\t\"categorical\":[2,3,4]\n",
    "\t\t},\n",
    "    \"model|MPNN|hidden_dim\":1,\n",
    "    \"model|MPNN|activation\":\"LeakyReLU\",\n",
    "    \"model|MPNN|activation_alpha\":0.01,\n",
    "    \"model|MPNN|use_bias\":0,\n",
    "    \"model|MPNN|include_edge_feat\":1,\n",
    "    \"model|MPNN|layer_num\":{\n",
    "\t\t\"categorical\":[3,4,5]\n",
    "\t\t},\n",
    "    \"model|MPNN|repeat_msgANDhidden_layer\":1,\n",
    "\n",
    "    \"model|dense_E1|units\":1,\n",
    "    \"model|dense_E1|activation\":\"sigmoid\",\n",
    "    \"model|dense_E1|use_bias\":0,\n",
    "    \"model|dense_E1|kernel_regularizer\":\"l2\",\n",
    "    \"model|dense_E1|kernel_constraint\":\"None\",\n",
    "\n",
    "    \"model|dense_E2|units\":1,\n",
    "    \"model|dense_E2|activation\":\"sigmoid\",\n",
    "    \"model|dense_E2|use_bias\":0,\n",
    "    \"model|dense_E2|kernel_regularizer\":\"l2\",\n",
    "    \"model|dense_E2|kernel_constraint\":\"NonNeg\",\n",
    "    \n",
    "    \"model|dense_output|units\":1,\n",
    "    \"model|dense_output|activation\":\"linear\",\n",
    "    \"model|dense_output|use_bias\":0,\n",
    "    \"model|dense_output|kernel_regularizer\":\"None\",\n",
    "    \"model|dense_output|kernel_constraint\":\"NonNeg\"\n",
    "}\n",
    "'''\n",
    "\n",
    "print(_input_filename)\n",
    "with open(os.path.join(os.getcwd(),_input_filename),'w+') as output:\n",
    "  output.write(_input_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_new_filenames = [_input_filename]\n",
    "_devid = None\n",
    "_cont_study = None\n",
    "_study_log = None\n",
    "_test_mode =  None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-22 14:11:33,094]\u001b[0m A new study created in memory with name: no-name-c7bd51c4-cb54-4249-ad8c-a133e83072e8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...creating new Study\n",
      "{\n",
      " \"cells\": [\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 43,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"# this .ipynb file is modified from _optuna_VALI.py for demonstration\\n\",\n",
      "    \"import optuna\\n\",\n",
      "    \"import tensorflow as tf\\n\",\n",
      "    \"from tensorflow.keras.layers.experimental import preprocessing\\n\",\n",
      "    \"import dgl\\n\",\n",
      "    \"\\n\",\n",
      "    \"import pandas as pd\\n\",\n",
      "    \"import numpy as np\\n\",\n",
      "    \"import os\\n\",\n",
      "    \"import time\\n\",\n",
      "    \"import pickle\\n\",\n",
      "    \"import datetime , time\\n\",\n",
      "    \"import json\\n\",\n",
      "    \"import copy\\n\",\n",
      "    \"import math\\n\",\n",
      "    \"import argparse\\n\",\n",
      "    \"import glob\\n\",\n",
      "    \"import random\\n\",\n",
      "    \"import sys\\n\",\n",
      "    \"\\n\",\n",
      "    \"from sklearn.linear_model import LinearRegression\\n\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 44,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"try:\\n\",\n",
      "    \"    __file__\\n\",\n",
      "    \"except:\\n\",\n",
      "    \"    __file__ = os.path.join(os.getcwd(),'_optuna_VALI.ipynb')\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"_DEMO = True # added for this folder only\\n\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 45,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"\\n\",\n",
      "    \"currentdir = os.path.dirname(os.path.realpath(__file__))\\n\",\n",
      "    \"sys.path.insert(1, currentdir+\\\"/../../Model\\\")\\n\",\n",
      "    \"sys.path.insert(1, currentdir+\\\"/../..\\\")\\n\",\n",
      "    \"_func = __import__(\\\"_func\\\")\\n\",\n",
      "    \"\\n\",\n",
      "    \"#________________________________________________________________________________________________________\\n\",\n",
      "    \"\\n\",\n",
      "    \"# get cpu/gpu\\n\",\n",
      "    \"tf.debugging.set_log_device_placement(True)\\n\",\n",
      "    \"_detect_gpus = tf.config.list_logical_devices('GPU')\\n\",\n",
      "    \"if len(_detect_gpus)==0:\\n\",\n",
      "    \"    _device_type=\\\"cpu\\\"\\n\",\n",
      "    \"else:\\n\",\n",
      "    \"    _device_type=\\\"gpu\\\"\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"#________________________________________________________________________________________________________\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Helper function to get data\\n\",\n",
      "    \"get_data =  _func.get_data\\n\",\n",
      "    \"\\n\",\n",
      "    \"# model defining\\n\",\n",
      "    \"def create_model(trial_setup_dict): # normalizer always = None\\n\",\n",
      "    \"    print(\\\"Model ...\\\")\\n\",\n",
      "    \"        \\n\",\n",
      "    \"    GNN_model = __import__(trial_setup_dict['modeltype_pyfilename'])\\n\",\n",
      "    \"\\n\",\n",
      "    \"    model = GNN_model.MODEL(trial_setup_dict['model'])\\n\",\n",
      "    \"\\n\",\n",
      "    \"    return model\\n\",\n",
      "    \"\\n\",\n",
      "    \"def create_optimizer(trial_setup_dict):\\n\",\n",
      "    \"    print(\\\"Optimizer ...\\\")\\n\",\n",
      "    \"    optimizer_chosen = trial_setup_dict[\\\"optimizer_chosen\\\"]\\n\",\n",
      "    \"    kwargs = trial_setup_dict[optimizer_chosen]\\n\",\n",
      "    \"\\n\",\n",
      "    \"    optimizer = getattr(tf.optimizers, optimizer_chosen)(**kwargs)\\n\",\n",
      "    \"    return optimizer\\n\",\n",
      "    \"\\n\",\n",
      "    \"def learn(model, \\n\",\n",
      "    \"            optimizer, \\n\",\n",
      "    \"            train_dataset, \\n\",\n",
      "    \"            loss_mode,\\n\",\n",
      "    \"            mode=\\\"eval\\\", \\n\",\n",
      "    \"            feature_type=\\\"normal\\\" , \\n\",\n",
      "    \"            training_seed = None, \\n\",\n",
      "    \"            _device_id=\\\"0\\\"):\\n\",\n",
      "    \"    MAE = tf.keras.losses.MeanAbsoluteError()\\n\",\n",
      "    \"    MAPE = tf.keras.losses.MeanAbsolutePercentageError()\\n\",\n",
      "    \"\\n\",\n",
      "    \"    epoch_loss = 0\\n\",\n",
      "    \"    total = 0\\n\",\n",
      "    \"\\n\",\n",
      "    \"    if mode == \\\"train\\\" and training_seed == None:\\n\",\n",
      "    \"        print(\\\"ERROR, require to set a training seed\\\")\\n\",\n",
      "    \"        exit()\\n\",\n",
      "    \"\\n\",\n",
      "    \"    # turn off\\n\",\n",
      "    \"    # expect the input to be sth like _1\\n\",\n",
      "    \"    #_device_id=_device_id.replace(\\\"_\\\",\\\"\\\")\\n\",\n",
      "    \"    #if _device_id==\\\"\\\":\\n\",\n",
      "    \"    #    _device_id = \\\"0\\\"\\n\",\n",
      "    \"    _device_id = \\\"0\\\"\\n\",\n",
      "    \"    _device = \\\"/{0}:{1}\\\".format(_device_type,_device_id)\\n\",\n",
      "    \"    with tf.device(_device):\\n\",\n",
      "    \"        for batch, (features, labels) in enumerate(train_dataset):\\n\",\n",
      "    \"            labels = tf.convert_to_tensor(labels)\\n\",\n",
      "    \"            with tf.GradientTape() as tape:\\n\",\n",
      "    \"                # set seed for reproducibility\\n\",\n",
      "    \"                if mode == \\\"train\\\":\\n\",\n",
      "    \"                    tf.random.set_seed(training_seed)\\n\",\n",
      "    \"                logits = model(features, training=(mode==\\\"train\\\"), _device=_device)\\n\",\n",
      "    \"                if loss_mode == 'MAE':\\n\",\n",
      "    \"                    loss = MAE(logits,labels)\\n\",\n",
      "    \"                elif loss_mode == 'MAPE':\\n\",\n",
      "    \"                    loss = MAPE(labels,logits)\\n\",\n",
      "    \"                \\n\",\n",
      "    \"            if feature_type == \\\"normal\\\":\\n\",\n",
      "    \"                current_batch_size = len(features)\\n\",\n",
      "    \"            else:\\n\",\n",
      "    \"                current_batch_size = len(features[0])\\n\",\n",
      "    \"\\n\",\n",
      "    \"            total += current_batch_size\\n\",\n",
      "    \"            epoch_loss = (epoch_loss*(total-current_batch_size) + loss*current_batch_size)/total\\n\",\n",
      "    \"            gradients = tape.gradient(loss, model.trainable_variables)\\n\",\n",
      "    \"            if (mode==\\\"train\\\"):\\n\",\n",
      "    \"                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"    if mode == \\\"eval\\\":\\n\",\n",
      "    \"        print(\\\"Eval ...\\\")\\n\",\n",
      "    \"    elif mode == \\\"eval_silent\\\":\\n\",\n",
      "    \"        pass\\n\",\n",
      "    \"    return epoch_loss\\n\",\n",
      "    \"\\n\",\n",
      "    \"def evaluate(model, optimizer, test_dataset,trial_setup_dict):\\n\",\n",
      "    \"\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    method = trial_setup_dict[\\\"eval_method\\\"]\\n\",\n",
      "    \"\\n\",\n",
      "    \"    if method == \\\"default\\\":\\n\",\n",
      "    \"        loss_mode=trial_setup_dict['loss_mode']\\n\",\n",
      "    \"        return learn(model, optimizer, test_dataset, loss_mode, \\\"eval_silent\\\", feature_type = 'split' )\\n\",\n",
      "    \"\\n\",\n",
      "    \"    elif method == \\\"r_square\\\":\\n\",\n",
      "    \"        # pretreatment\\n\",\n",
      "    \"        test_features = np.concatenate([x for _, (x, _) in enumerate(test_dataset)])\\n\",\n",
      "    \"        test_values = np.concatenate([y for _, (_, y) in enumerate(test_dataset)])\\n\",\n",
      "    \"        pred = model.predict(np.array(test_features))\\n\",\n",
      "    \"        real = test_values\\n\",\n",
      "    \"        _pred = np.array(pred).reshape(-1, 1)\\n\",\n",
      "    \"        _real = np.array(real)\\n\",\n",
      "    \"        linreg = LinearRegression(normalize=False,fit_intercept=True).fit(_pred,_real)\\n\",\n",
      "    \"        linreg.coef_ = np.array([1])\\n\",\n",
      "    \"        linreg.intercept_ = 0 \\n\",\n",
      "    \"        get_r_square = linreg.score(_pred, _real)\\n\",\n",
      "    \"        return get_r_square\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"def objective(trial,optuna_setup_dict, mode=\\\"trial\\\"):\\n\",\n",
      "    \"    print(\\\"Objective ...\\\")\\n\",\n",
      "    \"\\n\",\n",
      "    \"    if mode==\\\"trial\\\":\\n\",\n",
      "    \"        #initialize trial\\n\",\n",
      "    \"        for k,v in optuna_setup_dict.items():\\n\",\n",
      "    \"            # treat numbers with range\\n\",\n",
      "    \"            if type(v)==dict:\\n\",\n",
      "    \"                v_type = list(v.keys())[0]\\n\",\n",
      "    \"                if v_type == 'categorical':\\n\",\n",
      "    \"                    v_choices = v[v_type]\\n\",\n",
      "    \"                    #if type(v_choices) == list:\\n\",\n",
      "    \"                    trial.suggest_categorical(k,v_choices )\\n\",\n",
      "    \"                    #else:\\n\",\n",
      "    \"                    #    print(\\\"##### syntax ERROR #####, at:\\\",k,v)\\n\",\n",
      "    \"                elif v_type == 'discrete_uniform':\\n\",\n",
      "    \"                    v_low = v[v_type][0]\\n\",\n",
      "    \"                    v_high = v[v_type][1]\\n\",\n",
      "    \"                    v_q = v[v_type][2]  \\n\",\n",
      "    \"                    trial.suggest_discrete_uniform(k, v_low,v_high,v_q)\\n\",\n",
      "    \"                elif v_type in ['float','int']:\\n\",\n",
      "    \"                    v_low = v[v_type][0]\\n\",\n",
      "    \"                    v_high = v[v_type][1]\\n\",\n",
      "    \"                    if v_type == 'float': v_step = None\\n\",\n",
      "    \"                    elif v_type == 'int': v_step = 1\\n\",\n",
      "    \"                    v_log = False\\n\",\n",
      "    \"                    if len(v[v_type]) > 2:\\n\",\n",
      "    \"                        v_optionals = v[v_type][2]\\n\",\n",
      "    \"                        v_step = v_optionals.get('step')\\n\",\n",
      "    \"                        v_log = v_optionals.get('log')\\n\",\n",
      "    \"                        if v_log == None:\\n\",\n",
      "    \"                            v_log = False\\n\",\n",
      "    \"                    if v_step and v_log:\\n\",\n",
      "    \"                        print(\\\"ERROR at {}, step and log cannot be used together:\\\".foramt(v_type),k,v)\\n\",\n",
      "    \"                        exit()\\n\",\n",
      "    \"                    else:\\n\",\n",
      "    \"                        if v_type == 'float':\\n\",\n",
      "    \"                            trial.suggest_float(k, v_low, v_high, step=v_step,log=v_log)\\n\",\n",
      "    \"                        elif v_type == 'int':\\n\",\n",
      "    \"                            trial.suggest_int(k, v_low, v_high, step=v_step,log=v_log)\\n\",\n",
      "    \"                elif v_type in ['loguniform','uniform']:\\n\",\n",
      "    \"                    v_low = v[v_type][0]\\n\",\n",
      "    \"                    v_high = v[v_type][1]\\n\",\n",
      "    \"                    if v_type == 'loguniform':\\n\",\n",
      "    \"                        trial.suggest_loguniform(k, v_low, v_high)\\n\",\n",
      "    \"                    elif v_type == 'uniform':\\n\",\n",
      "    \"                        trial.suggest_uniform(k, v_low, v_high)\\n\",\n",
      "    \"                else:\\n\",\n",
      "    \"                    print(\\\"ERROR v_type {}\\\",k,v)\\n\",\n",
      "    \"                print(\\\"setting {} to param {} \\\".format(v,k))\\n\",\n",
      "    \"            else:\\n\",\n",
      "    \"                print(\\\"setting {} to attr {} \\\".format(v,k))\\n\",\n",
      "    \"                trial.set_user_attr(k, v)\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"        def merge(d1, d2):\\n\",\n",
      "    \"            for k in d2:\\n\",\n",
      "    \"                if k in d1 and isinstance(d1[k], dict) and isinstance(d2[k], dict):\\n\",\n",
      "    \"                    merge(d1[k], d2[k])\\n\",\n",
      "    \"                else:\\n\",\n",
      "    \"                    d1[k] = d2[k]\\n\",\n",
      "    \"        def copy_merge(d1,d2):\\n\",\n",
      "    \"            _d1 = copy.deepcopy(d1)\\n\",\n",
      "    \"            _d2 = copy.deepcopy(d2)\\n\",\n",
      "    \"            merge(_d1,_d2)\\n\",\n",
      "    \"            return _d1\\n\",\n",
      "    \"\\n\",\n",
      "    \"        trial_setup_dict = {}\\n\",\n",
      "    \"        for _dict in [trial.user_attrs , trial.params]:\\n\",\n",
      "    \"            for k,v in _dict.items():\\n\",\n",
      "    \"                if \\\"|\\\" in k:\\n\",\n",
      "    \"                    _kname_list = k.split(\\\"|\\\")\\n\",\n",
      "    \"                    _kname_list.reverse()\\n\",\n",
      "    \"                    _dict = v\\n\",\n",
      "    \"                    for _kname in _kname_list:\\n\",\n",
      "    \"                        _dict = {_kname:_dict }\\n\",\n",
      "    \"                    trial_setup_dict = copy_merge(trial_setup_dict,_dict)\\n\",\n",
      "    \"                else:\\n\",\n",
      "    \"                    trial_setup_dict[k] = v\\n\",\n",
      "    \"    elif mode==\\\"simple_train\\\" or  mode==\\\"cont_train\\\":\\n\",\n",
      "    \"        trial_setup_dict =  copy.deepcopy(optuna_setup_dict)\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"    # choose optuna mode\\n\",\n",
      "    \"    opttrain_mode = trial_setup_dict['opttrain_mode']\\n\",\n",
      "    \"    #opttrain = __import__(opttrain_mode)\\n\",\n",
      "    \"\\n\",\n",
      "    \"    # refine trial inputs\\n\",\n",
      "    \"    trial_setup_dict['input_ndata_dim'] = len(trial_setup_dict['input_ndata_list'])\\n\",\n",
      "    \"    trial_setup_dict['input_edata_dim'] = len(trial_setup_dict['input_edata_list'])\\n\",\n",
      "    \"    devid_suffix = trial_setup_dict['devid_suffix']\\n\",\n",
      "    \"\\n\",\n",
      "    \"    #________________________________________________________________________________________________________\\n\",\n",
      "    \"    # define saving directory\\n\",\n",
      "    \"    z = datetime.datetime.now()\\n\",\n",
      "    \"    study_foldername = trial_setup_dict['study_foldername']\\n\",\n",
      "    \"    if mode != \\\"cont_train\\\": #trial_setup_dict.get('modeltrained_foldername') == None:\\n\",\n",
      "    \"        if _DEMO:\\n\",\n",
      "    \"            modeltrained_foldername = \\\"_{0}_DEMO_s{1:02d}_ms{2:06d}_model\\\".format(study_foldername,z.second,z.microsecond).replace(\\\"-\\\",\\\"\\\")\\n\",\n",
      "    \"        else:\\n\",\n",
      "    \"            pass\\n\",\n",
      "    \"            #modeltrained_foldername = \\\"_{0}_{1}_{2:02d}_{3:02d}_s{4:02d}_ms{5:06d}_model\\\".format(study_foldername,z.date(),z.hour,z.minute,z.second,z.microsecond).replace(\\\"-\\\",\\\"\\\")\\n\",\n",
      "    \"        trial_setup_dict['modeltrained_foldername'] = modeltrained_foldername\\n\",\n",
      "    \"    else:\\n\",\n",
      "    \"        loss_mode = trial_setup_dict['loss_mode']\\n\",\n",
      "    \"        modeltrained_foldername_prev = trial_setup_dict['modeltrained_foldername']\\n\",\n",
      "    \"        trial_setup_dict['modeltrained_foldername'] = modeltrained_foldername_prev+\\\"_##{}_CONT\\\".format(loss_mode)\\n\",\n",
      "    \"        modeltrained_foldername = trial_setup_dict['modeltrained_foldername']\\n\",\n",
      "    \"    saving_dir = currentdir+\\\"/\\\"+modeltrained_foldername\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"    stdoutOrigin=sys.stdout \\n\",\n",
      "    \"    if trial_setup_dict.get('_test_mode') == \\\"1\\\":\\n\",\n",
      "    \"        log_filename = \\\"_TEST_trainlog\\\"\\n\",\n",
      "    \"    else:\\n\",\n",
      "    \"        log_filename = modeltrained_foldername+\\\"_trainlog\\\"\\n\",\n",
      "    \"\\n\",\n",
      "    \"    sys.stdout = open(log_filename, \\\"w\\\")\\n\",\n",
      "    \"\\n\",\n",
      "    \"    #________________________________________________________________________________________________________\\n\",\n",
      "    \"    # gen seed if not specified\\n\",\n",
      "    \"    if trial_setup_dict.get('training_seed') == None:\\n\",\n",
      "    \"        trial_setup_dict['training_seed'] = random.randint(0,9999)\\n\",\n",
      "    \"    training_seed = trial_setup_dict['training_seed']\\n\",\n",
      "    \"\\n\",\n",
      "    \"    print('trial_setup_dict')\\n\",\n",
      "    \"    print(json.dumps(trial_setup_dict,indent=4))\\n\",\n",
      "    \"\\n\",\n",
      "    \"    print('training_seed=',training_seed)\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"    #________________________________________________________________________________________________________\\n\",\n",
      "    \"    trial_begin = time.time()\\n\",\n",
      "    \"    # Get train/test data.\\n\",\n",
      "    \"    _output_data_dict = _func.get_data(\\n\",\n",
      "    \"        trial_setup_dict,\\n\",\n",
      "    \"        batch_size=trial_setup_dict['batch_size'],\\n\",\n",
      "    \"        test_split=trial_setup_dict['test_split'],\\n\",\n",
      "    \"        vali_split=trial_setup_dict['vali_split'],\\n\",\n",
      "    \"        currentdir = currentdir,\\n\",\n",
      "    \"        return_indexes=True\\n\",\n",
      "    \"    )\\n\",\n",
      "    \"    dummy_train_dataset         = _output_data_dict['stage1_train_dataset']\\n\",\n",
      "    \"    train_dataset               = _output_data_dict['train_dataset']\\n\",\n",
      "    \"    dummy_vali_dataset          = _output_data_dict['stage1_vali_dataset']\\n\",\n",
      "    \"    vali_dataset                = _output_data_dict['vali_dataset']\\n\",\n",
      "    \"    dummy_test_dataset          = _output_data_dict['stage1_test_dataset']\\n\",\n",
      "    \"    test_dataset                = _output_data_dict['test_dataset']\\n\",\n",
      "    \"\\n\",\n",
      "    \"    node_num_info               = _output_data_dict['node_num_info']\\n\",\n",
      "    \"    train_vali_test_indexes     = _output_data_dict['train_vali_test_indexes']\\n\",\n",
      "    \"\\n\",\n",
      "    \"    if trial_setup_dict.get('node_num_info') == None:\\n\",\n",
      "    \"        trial_setup_dict['node_num_info'] = node_num_info\\n\",\n",
      "    \"\\n\",\n",
      "    \"    # Build model and optimizer.\\n\",\n",
      "    \"    model = create_model(trial_setup_dict)\\n\",\n",
      "    \"    optimizer = create_optimizer(trial_setup_dict)\\n\",\n",
      "    \"\\n\",\n",
      "    \"    # setup for saving models\\n\",\n",
      "    \"    if mode==\\\"cont_train\\\":\\n\",\n",
      "    \"        print(\\\"...Restoring checkpoint\\\")\\n\",\n",
      "    \"        # restore ckpt\\n\",\n",
      "    \"        ckpt = tf.train.Checkpoint(\\n\",\n",
      "    \"            step=tf.Variable(1), optimizer=optimizer, net=model\\n\",\n",
      "    \"        )\\n\",\n",
      "    \"        saving_dir_prev = currentdir+\\\"/{}/{}\\\".format(modeltrained_foldername_prev,modeltrained_foldername_prev)\\n\",\n",
      "    \"        _manager = tf.train.CheckpointManager(ckpt, saving_dir_prev, max_to_keep=3)\\n\",\n",
      "    \"        ckpt.restore(_manager.latest_checkpoint)\\n\",\n",
      "    \"        if _manager.latest_checkpoint:\\n\",\n",
      "    \"            print(\\\"Restored from {}\\\".format(_manager.latest_checkpoint))\\n\",\n",
      "    \"    else:\\n\",\n",
      "    \"        # create ckpt\\n\",\n",
      "    \"        ckpt = tf.train.Checkpoint(\\n\",\n",
      "    \"            step=tf.Variable(1), optimizer=optimizer, net=model\\n\",\n",
      "    \"        )\\n\",\n",
      "    \"    manager = tf.train.CheckpointManager(ckpt, saving_dir+\\\"/{}\\\".format(saving_dir.split(\\\"/\\\")[-1]\\n\",\n",
      "    \"    ) \\n\",\n",
      "    \"        , max_to_keep=3)\\n\",\n",
      "    \"\\n\",\n",
      "    \"    # gen cutoff\\n\",\n",
      "    \"    stage1_cutoff = trial_setup_dict['stage1_cutoff']\\n\",\n",
      "    \"    stage1_skip = trial_setup_dict.get('stage1_skip')\\n\",\n",
      "    \"    stage2_converge = trial_setup_dict.get('stage2_converge')\\n\",\n",
      "    \"\\n\",\n",
      "    \"    stage2_fluctuate_dv = trial_setup_dict['stage2_fluctuate_dv']\\n\",\n",
      "    \"    stage2_fluctuate_pc = trial_setup_dict['stage2_fluctuate_pc']\\n\",\n",
      "    \"    if stage2_converge == None:\\n\",\n",
      "    \"        stage2_converge = 0.0\\n\",\n",
      "    \"    loss_mode = trial_setup_dict['loss_mode']\\n\",\n",
      "    \"\\n\",\n",
      "    \"    loss_record_dict = {1:{},2:{}}\\n\",\n",
      "    \"    # itterate training sequence:\\n\",\n",
      "    \"    for _stage in [1,2]:\\n\",\n",
      "    \"        max_epoch_num = trial_setup_dict['max_epoch_num']\\n\",\n",
      "    \"\\n\",\n",
      "    \"        if _stage==1:\\n\",\n",
      "    \"            print(\\\"################## STAGE  1 #######################\\\")\\n\",\n",
      "    \"            model.stopuseArrheniusEq()\\n\",\n",
      "    \"            trial_train_dataset = dummy_train_dataset\\n\",\n",
      "    \"            trial_vali_dataset = dummy_vali_dataset\\n\",\n",
      "    \"            trial_test_dataset = dummy_test_dataset\\n\",\n",
      "    \"            if stage1_skip:\\n\",\n",
      "    \"                print(\\\"Skipping stage 1\\\")\\n\",\n",
      "    \"                continue\\n\",\n",
      "    \"        if _stage==2:\\n\",\n",
      "    \"            print(\\\"################## STAGE  2 #######################\\\")\\n\",\n",
      "    \"            model.startuseArrheniusEq()\\n\",\n",
      "    \"            trial_train_dataset = train_dataset\\n\",\n",
      "    \"            trial_vali_dataset = vali_dataset\\n\",\n",
      "    \"            trial_test_dataset = test_dataset\\n\",\n",
      "    \"        print(\\\"use Arrhenius:\\\",model.useArrheniusEq)\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"        print(\\\"Epoch ( [train|valid|test] ): \\\", end =\\\" \\\", flush=True)\\n\",\n",
      "    \"        # Training and testing cycle.\\n\",\n",
      "    \"        last10_loss = np.array([i*10 for i in range(1,11)]) # [10,... 100]\\n\",\n",
      "    \"        loss_record = {\\\"loss\\\":[],\\\"vali_loss\\\":[],\\\"test_loss\\\":[]}\\n\",\n",
      "    \"\\n\",\n",
      "    \"        for i in range(max_epoch_num):\\n\",\n",
      "    \"            loss = learn(model, optimizer, trial_train_dataset, loss_mode, \\\"train\\\", feature_type = 'split' , training_seed = training_seed) #, _device_id=devid_suffix)\\n\",\n",
      "    \"            loss_record['loss'].append(round(float(loss),7))\\n\",\n",
      "    \"            \\n\",\n",
      "    \"            vali_loss = learn(model, optimizer, trial_vali_dataset, loss_mode, \\\"eval_silent\\\", feature_type = 'split'\\n\",\n",
      "    \"            )\\n\",\n",
      "    \"            loss_record['vali_loss'].append(round(float(vali_loss),7))\\n\",\n",
      "    \"\\n\",\n",
      "    \"            test_loss = learn(model, optimizer, trial_test_dataset, loss_mode, \\\"eval_silent\\\", feature_type = 'split'\\n\",\n",
      "    \"            )\\n\",\n",
      "    \"            loss_record['test_loss'].append(round(float(test_loss),7))\\n\",\n",
      "    \"\\n\",\n",
      "    \"    \\n\",\n",
      "    \"            ckpt.step.assign_add(1)\\n\",\n",
      "    \"\\n\",\n",
      "    \"            if _stage==1 and round(float(loss),7) <=stage1_cutoff:\\n\",\n",
      "    \"                break\\n\",\n",
      "    \"\\n\",\n",
      "    \"            if _stage==2:\\n\",\n",
      "    \"                last10_loss_prev = last10_loss.copy()\\n\",\n",
      "    \"                last10_loss = last10_loss[1:]\\n\",\n",
      "    \"                last10_loss = np.append(last10_loss,[loss])\\n\",\n",
      "    \"\\n\",\n",
      "    \"                delta_loss = [last10_loss[_i]-last10_loss_prev[_i] for _i in range(len(last10_loss_prev))]\\n\",\n",
      "    \"                fluctuate_pc = sum([  0 > delta_loss[_i]*delta_loss[_i+1] for _i in range(len(delta_loss)-1)]) / (len(delta_loss)-1)\\n\",\n",
      "    \"                avg_dv = np.mean(np.abs(delta_loss))\\n\",\n",
      "    \"                \\n\",\n",
      "    \"                # break if converged\\n\",\n",
      "    \"                if avg_dv <= stage2_converge:\\n\",\n",
      "    \"                    print(\\\"...converged\\\")\\n\",\n",
      "    \"                    break\\n\",\n",
      "    \"                # break if fluctuate too much\\n\",\n",
      "    \"                if fluctuate_pc >= stage2_fluctuate_pc and avg_dv >= stage2_fluctuate_dv:\\n\",\n",
      "    \"                    print(\\\"...fluctuation too high:\\\")\\n\",\n",
      "    \"                    print(\\\"fluctuate_pc:\\\",fluctuate_pc)\\n\",\n",
      "    \"                    print(\\\"stage2_fluctuate_dv:\\\",avg_dv)\\n\",\n",
      "    \"                    break\\n\",\n",
      "    \"\\n\",\n",
      "    \"            if i%10 == 0:\\n\",\n",
      "    \"                print(\\\"{} [{}|{}|{}]-> \\\".format(i,round(float(loss),7) , round(float(vali_loss),7), round(float(test_loss),7) ), end =\\\" \\\", flush=True)\\n\",\n",
      "    \"\\n\",\n",
      "    \"        print(\\\"{} [{}|{}|{}]-> \\\".format(i,round(float(loss),7) , round(float(vali_loss),7), round(float(test_loss),7) ), end =\\\" \\\", flush=True)\\n\",\n",
      "    \"        print(\\\"END\\\")\\n\",\n",
      "    \"        if _stage==2:\\n\",\n",
      "    \"            print(\\\"avg_dv out of {0}: {1}\\\".format(len(last10_loss),avg_dv))\\n\",\n",
      "    \"        loss_record_dict[_stage] = loss_record\\n\",\n",
      "    \"\\n\",\n",
      "    \"    eval_result = evaluate(model, optimizer, test_dataset, trial_setup_dict)\\n\",\n",
      "    \"    trial_end = time.time()\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"    # save model\\n\",\n",
      "    \"    manager.save()\\n\",\n",
      "    \"    with open(saving_dir+\\\"/{}_setting\\\".format(saving_dir.split(\\\"/\\\")[-1]\\n\",\n",
      "    \"    ),\\\"w+\\\") as f:\\n\",\n",
      "    \"        f.write(json.dumps(trial_setup_dict,indent=4).replace(\\\"\\\\'\\\",\\\"\\\\\\\"\\\"))\\n\",\n",
      "    \"    with open(saving_dir+\\\"/{}_train_vali_test_indexes\\\".format(saving_dir.split(\\\"/\\\")[-1]\\n\",\n",
      "    \"    ),\\\"w+\\\") as f:\\n\",\n",
      "    \"        f.write(json.dumps(train_vali_test_indexes,indent=4).replace(\\\"\\\\'\\\",\\\"\\\\\\\"\\\"))\\n\",\n",
      "    \"    with open(saving_dir+\\\"/loss_record\\\",\\\"w+\\\") as f:\\n\",\n",
      "    \"        f.write(json.dumps(loss_record_dict,indent=4).replace(\\\"\\\\'\\\",\\\"\\\\\\\"\\\"))\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"    # save other files\\n\",\n",
      "    \"    with open(__file__,\\\"r\\\") as input:\\n\",\n",
      "    \"        raw_pyfile_record = input.read()\\n\",\n",
      "    \"\\n\",\n",
      "    \"    with open(saving_dir+\\\"/rawpyrecord\\\",\\\"w+\\\") as f:\\n\",\n",
      "    \"        f.write(raw_pyfile_record)\\n\",\n",
      "    \"        \\n\",\n",
      "    \"    print(\\\"This trial used {} seconds\\\".format(trial_end-trial_begin))\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    # save logfiles\\n\",\n",
      "    \"    sys.stdout.close()\\n\",\n",
      "    \"    sys.stdout=stdoutOrigin\\n\",\n",
      "    \"    with open(log_filename, \\\"r\\\") as input:\\n\",\n",
      "    \"        _log = input.read()\\n\",\n",
      "    \"    with open(saving_dir+\\\"/trainlog\\\", \\\"w+\\\") as output:\\n\",\n",
      "    \"        output.write(_log)\\n\",\n",
      "    \"    os.remove(log_filename)\\n\",\n",
      "    \"\\n\",\n",
      "    \"    return eval_result\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"def search(optuna_setup_dict,study):\\n\",\n",
      "    \"\\n\",\n",
      "    \"    print(f\\\"Sampler used is {study.sampler.__class__.__name__}\\\")\\n\",\n",
      "    \"\\n\",\n",
      "    \"    f = lambda y: objective(y,optuna_setup_dict)\\n\",\n",
      "    \"\\n\",\n",
      "    \"    study_filename = optuna_setup_dict[\\\"study_foldername\\\"]\\n\",\n",
      "    \"    study_dir = os.path.join(currentdir,study_filename,\\\"study\\\")\\n\",\n",
      "    \"\\n\",\n",
      "    \"    total_n_trials = optuna_setup_dict[\\\"study_total_n_trials\\\"]-len(study.trials)\\n\",\n",
      "    \"    num_per_batch =  optuna_setup_dict[\\\"study_num_per_batch\\\"]\\n\",\n",
      "    \"    total_batch_count = math.ceil(total_n_trials/num_per_batch)\\n\",\n",
      "    \"\\n\",\n",
      "    \"    record_study_log = optuna_setup_dict[\\\"study_log_input\\\"]\\n\",\n",
      "    \"    if record_study_log:\\n\",\n",
      "    \"        study_log_inputdir = os.path.join(currentdir,optuna_setup_dict[\\\"study_log_input\\\"])\\n\",\n",
      "    \"        study_log_outputdir = os.path.join(currentdir,optuna_setup_dict[\\\"study_log_output\\\"])\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    for _i in range(1,total_batch_count+1):\\n\",\n",
      "    \"\\n\",\n",
      "    \"        n_trials = 3\\n\",\n",
      "    \"        if _i*num_per_batch > total_n_trials :\\n\",\n",
      "    \"            n_trials = total_n_trials%num_per_batch\\n\",\n",
      "    \"\\n\",\n",
      "    \"        if os.path.isfile(study_dir):\\n\",\n",
      "    \"            with open(study_dir, 'rb') as _study_input:\\n\",\n",
      "    \"                study = pickle.load(_study_input)\\n\",\n",
      "    \"\\n\",\n",
      "    \"        study.optimize(f, n_trials=n_trials)\\n\",\n",
      "    \"\\n\",\n",
      "    \"        # save study\\n\",\n",
      "    \"        pickle.dump(study, open(study_dir, \\\"wb\\\"))\\n\",\n",
      "    \"\\n\",\n",
      "    \"        print(\\\"Number of finished trials: \\\", len(study.trials))\\n\",\n",
      "    \"\\n\",\n",
      "    \"        print(\\\"Best trial:\\\")\\n\",\n",
      "    \"        trial = study.best_trial\\n\",\n",
      "    \"\\n\",\n",
      "    \"        print(\\\"  Value: \\\", trial.value)\\n\",\n",
      "    \"\\n\",\n",
      "    \"        print(\\\"  Params: \\\")\\n\",\n",
      "    \"        for key, value in trial.params.items():\\n\",\n",
      "    \"            print(\\\"    {}: {}\\\".format(key, value)) \\n\",\n",
      "    \"\\n\",\n",
      "    \"        if record_study_log:\\n\",\n",
      "    \"            with open(study_log_inputdir,\\\"r\\\") as _study_log_input:\\n\",\n",
      "    \"                study_log = _study_log_input.read()\\n\",\n",
      "    \"            with open(study_log_outputdir,\\\"w+\\\") as _study_log_output:\\n\",\n",
      "    \"                _study_log_output.write(study_log)\\n\",\n",
      "    \"        \\n\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 46,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"_oos_ligid = 1\\n\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 47,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"_DEMO.inp\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"import json\\n\",\n",
      "    \"import random\\n\",\n",
      "    \"\\n\",\n",
      "    \"_input_filename = '_DEMO.inp'\\n\",\n",
      "    \"_input_json='''{\\n\",\n",
      "    \"    \\\"study_total_n_trials\\\":1,\\n\",\n",
      "    \"    \\\"study_num_per_batch\\\":1,\\n\",\n",
      "    \"    \\\"study_direction\\\":\\\"minimize\\\",\\n\",\n",
      "    \"    \\\"loss_mode\\\":\\\"MAE\\\",\\n\",\n",
      "    \"    \\\"eval_method\\\":\\\"default\\\",\\n\",\n",
      "    \"\\n\",\n",
      "    \"    \\\"modeltype_pyfilename\\\":\\\"GNN\\\",\\n\",\n",
      "    \"    \\\"opttrain_mode\\\":\\\"formula_GPU\\\",\\n\",\n",
      "    \"    \\\"dataset_edge(LIG)_filename\\\":\\\"_graphBiEDGE2DATA(LIG).csv\\\",\\n\",\n",
      "    \"    \\\"dataset_node(LIG)_filename\\\":\\\"_graphNODEDATA(LIG).csv\\\",\\n\",\n",
      "    \"    \\\"dataset(RCT)_filename\\\":\\\"_rawDATA(RCT).xlsx\\\",\\n\",\n",
      "    \"    \\\"dataset(RCTxLIG)_filename\\\":\\\"_rawDATA(RCTxLIG)_deltaE.xlsx\\\",\\n\",\n",
      "    \"    \\\"constant_node_num\\\":1,\\n\",\n",
      "    \"    \\\"input_ndata_list\\\":[\\\"boolean\\\"],\\n\",\n",
      "    \"    \\\"input_edata_list\\\":[\\\"weight\\\"],\\n\",\n",
      "    \"\\n\",\n",
      "    \"    \\\"test_split\\\":0.2,\\n\",\n",
      "    \"    \\\"vali_split\\\":0.2,\\n\",\n",
      "    \"\\n\",\n",
      "    \"    \\\"stage1_cutoff\\\":{\\n\",\n",
      "    \"\\t\\t\\\"categorical\\\":[5.0,6.0,7.0]\\n\",\n",
      "    \"\\t\\t},\\n\",\n",
      "    \"    \\\"stage2_converge\\\":1e-5,\\n\",\n",
      "    \"    \\\"stage2_fluctuate_pc\\\":0.8,\\n\",\n",
      "    \"    \\\"stage2_fluctuate_dv\\\":0.2,\\n\",\n",
      "    \"    \\\"optimizer_chosen\\\":\\\"Adam\\\",\\n\",\n",
      "    \"    \\\"Adam|learning_rate\\\":{\\n\",\n",
      "    \"\\t\\t\\\"categorical\\\":[0.0005,0.001,0.002,0.005]\\n\",\n",
      "    \"\\t\\t},\\n\",\n",
      "    \"    \\\"max_epoch_num\\\":100,\\n\",\n",
      "    \"    \\\"batch_size\\\":10,\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"    \\\"model|MPNN|message_dim\\\":{\\n\",\n",
      "    \"\\t\\t\\\"categorical\\\":[2,3,4]\\n\",\n",
      "    \"\\t\\t},\\n\",\n",
      "    \"    \\\"model|MPNN|hidden_dim\\\":1,\\n\",\n",
      "    \"    \\\"model|MPNN|activation\\\":\\\"LeakyReLU\\\",\\n\",\n",
      "    \"    \\\"model|MPNN|activation_alpha\\\":0.01,\\n\",\n",
      "    \"    \\\"model|MPNN|use_bias\\\":0,\\n\",\n",
      "    \"    \\\"model|MPNN|include_edge_feat\\\":1,\\n\",\n",
      "    \"    \\\"model|MPNN|layer_num\\\":{\\n\",\n",
      "    \"\\t\\t\\\"categorical\\\":[3,4,5]\\n\",\n",
      "    \"\\t\\t},\\n\",\n",
      "    \"    \\\"model|MPNN|repeat_msgANDhidden_layer\\\":1,\\n\",\n",
      "    \"\\n\",\n",
      "    \"    \\\"model|dense_E1|units\\\":1,\\n\",\n",
      "    \"    \\\"model|dense_E1|activation\\\":\\\"sigmoid\\\",\\n\",\n",
      "    \"    \\\"model|dense_E1|use_bias\\\":0,\\n\",\n",
      "    \"    \\\"model|dense_E1|kernel_regularizer\\\":\\\"l2\\\",\\n\",\n",
      "    \"    \\\"model|dense_E1|kernel_constraint\\\":\\\"None\\\",\\n\",\n",
      "    \"\\n\",\n",
      "    \"    \\\"model|dense_E2|units\\\":1,\\n\",\n",
      "    \"    \\\"model|dense_E2|activation\\\":\\\"sigmoid\\\",\\n\",\n",
      "    \"    \\\"model|dense_E2|use_bias\\\":0,\\n\",\n",
      "    \"    \\\"model|dense_E2|kernel_regularizer\\\":\\\"l2\\\",\\n\",\n",
      "    \"    \\\"model|dense_E2|kernel_constraint\\\":\\\"NonNeg\\\",\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    \\\"model|dense_output|units\\\":1,\\n\",\n",
      "    \"    \\\"model|dense_output|activation\\\":\\\"linear\\\",\\n\",\n",
      "    \"    \\\"model|dense_output|use_bias\\\":0,\\n\",\n",
      "    \"    \\\"model|dense_output|kernel_regularizer\\\":\\\"None\\\",\\n\",\n",
      "    \"    \\\"model|dense_output|kernel_constraint\\\":\\\"NonNeg\\\"\\n\",\n",
      "    \"}\\n\",\n",
      "    \"'''\\n\",\n",
      "    \"\\n\",\n",
      "    \"print(_input_filename)\\n\",\n",
      "    \"with open(os.path.join(os.getcwd(),_input_filename),'w+') as output:\\n\",\n",
      "    \"  output.write(_input_json)\\n\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 48,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"\\n\",\n",
      "    \"_new_filenames = [_input_filename]\\n\",\n",
      "    \"_devid = None\\n\",\n",
      "    \"_cont_study = None\\n\",\n",
      "    \"_study_log = None\\n\",\n",
      "    \"_test_mode =  None\\n\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 49,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stderr\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"\\u001b[32m[I 2023-04-22 14:10:27,379]\\u001b[0m A new study created in memory with name: no-name-fd2c2b1d-ffca-486f-b221-7c4b51d37ead\\u001b[0m\\n\"\n",
      "     ]\n",
      "    },\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"...creating new Study\\n\"\n",
      "     ]\n",
      "    },\n",
      "    {\n",
      "     \"ename\": \"FileNotFoundError\",\n",
      "     \"evalue\": \"[Errno 2] No such file or directory: '/Users/jacky/Desktop/Current_projects/ML_CHEM/Tools/GNN_optuna/_DEMO/Train_test/GNN_DEMO/_optuna_VALI.ipynb'\",\n",
      "     \"output_type\": \"error\",\n",
      "     \"traceback\": [\n",
      "      \"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\n",
      "      \"\\u001b[0;31mFileNotFoundError\\u001b[0m                         Traceback (most recent call last)\",\n",
      "      \"\\u001b[0;32m~/Desktop/Current_projects/ML_CHEM/Tools/GNN_optuna/_DEMO/Train_test/GNN_DEMO/_optuna_VALI.ipynb\\u001b[0m in \\u001b[0;36m<module>\\u001b[0;34m\\u001b[0m\\n\\u001b[1;32m     62\\u001b[0m \\u001b[0moptuna_setup_dict\\u001b[0m\\u001b[0;34m[\\u001b[0m\\u001b[0;34m'_test_mode'\\u001b[0m\\u001b[0;34m]\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mstr\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0m_test_mode\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m     63\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m---> 64\\u001b[0;31m \\u001b[0;32mwith\\u001b[0m \\u001b[0mopen\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0m__file__\\u001b[0m\\u001b[0;34m,\\u001b[0m\\u001b[0;34m\\\"r\\\"\\u001b[0m\\u001b[0;34m)\\u001b[0m \\u001b[0;32mas\\u001b[0m \\u001b[0minput\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m     65\\u001b[0m     \\u001b[0mprint\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0minput\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mread\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m     66\\u001b[0m \\u001b[0msearch\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0moptuna_setup_dict\\u001b[0m\\u001b[0;34m,\\u001b[0m\\u001b[0mstudy\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n",
      "      \"\\u001b[0;31mFileNotFoundError\\u001b[0m: [Errno 2] No such file or directory: '/Users/jacky/Desktop/Current_projects/ML_CHEM/Tools/GNN_optuna/_DEMO/Train_test/GNN_DEMO/_optuna_VALI.ipynb'\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"# get current filelog\\n\",\n",
      "    \"if _devid:\\n\",\n",
      "    \"    devid_suffix = \\\"_#{}\\\".format(_devid)\\n\",\n",
      "    \"else:\\n\",\n",
      "    \"    devid_suffix = \\\"\\\"\\n\",\n",
      "    \"\\n\",\n",
      "    \"if _cont_study:\\n\",\n",
      "    \"    print(\\\"...continuing Study\\\")\\n\",\n",
      "    \"    # get continue study file & setup filename\\n\",\n",
      "    \"    study_foldername = _cont_study\\n\",\n",
      "    \"    study_dir = os.path.join(currentdir,study_foldername)\\n\",\n",
      "    \"    if _test_mode != \\\"1\\\":\\n\",\n",
      "    \"        with open(os.path.join(study_dir,\\\"_setup.inp\\\"), \\\"r\\\") as input:\\n\",\n",
      "    \"            optuna_setup_dict = json.loads(str(input.read()))\\n\",\n",
      "    \"\\n\",\n",
      "    \"    # save folder name\\n\",\n",
      "    \"    print(optuna_setup_dict)\\n\",\n",
      "    \"    optuna_setup_dict[\\\"study_foldername\\\"] = study_foldername\\n\",\n",
      "    \"    # load study\\n\",\n",
      "    \"    with open(os.path.join(study_dir,\\\"study\\\"), 'rb') as _study_input:\\n\",\n",
      "    \"        study = pickle.load(_study_input)\\n\",\n",
      "    \"\\n\",\n",
      "    \"elif _new_filenames:\\n\",\n",
      "    \"    print(\\\"...creating new Study\\\")\\n\",\n",
      "    \"    # get setup filename\\n\",\n",
      "    \"    setup_filename = _new_filenames[0].replace(\\\".inp\\\",\\\"\\\")\\n\",\n",
      "    \"    with open(setup_filename+\\\".inp\\\", \\\"r\\\") as input:\\n\",\n",
      "    \"        optuna_setup_dict = json.loads(str(input.read()))\\n\",\n",
      "    \"\\n\",\n",
      "    \"    # create study folder\\n\",\n",
      "    \"    all_begin = time.time()\\n\",\n",
      "    \"    z = datetime.datetime.now()\\n\",\n",
      "    \"    if _DEMO:\\n\",\n",
      "    \"        study_foldername = \\\"Optuna_StudyResult_DEMO_{0}{1}\\\".format(z.minute,devid_suffix).replace(\\\"-\\\",\\\"\\\")\\n\",\n",
      "    \"    else:\\n\",\n",
      "    \"        study_foldername = \\\"Optuna_StudyResult_{0}_{1:02d}{2:02d}{3}\\\".format(z.date(),z.hour,z.minute,devid_suffix).replace(\\\"-\\\",\\\"\\\")\\n\",\n",
      "    \"    optuna_setup_dict[\\\"study_foldername\\\"] = study_foldername\\n\",\n",
      "    \"    study_dir = currentdir+\\\"/\\\"+study_foldername\\n\",\n",
      "    \"    if _test_mode != \\\"1\\\":\\n\",\n",
      "    \"        if not( os.path.exists(study_dir) ):\\n\",\n",
      "    \"            os.makedirs(study_dir)\\n\",\n",
      "    \"        with open(os.path.join(currentdir,study_foldername,\\\"_setup.inp\\\"), \\\"w+\\\") as output:\\n\",\n",
      "    \"            output.write(json.dumps(optuna_setup_dict,indent=4))\\n\",\n",
      "    \"\\n\",\n",
      "    \"    # gen seed for sampler\\n\",\n",
      "    \"    if optuna_setup_dict.get('sampler_seed') == None:\\n\",\n",
      "    \"        sampler_seed = random.randint(0,9999)\\n\",\n",
      "    \"        optuna_setup_dict['sampler_seed'] = sampler_seed\\n\",\n",
      "    \"    sampler_seed = optuna_setup_dict['sampler_seed']\\n\",\n",
      "    \"\\n\",\n",
      "    \"    # create study\\n\",\n",
      "    \"    direction = optuna_setup_dict['study_direction']\\n\",\n",
      "    \"    sampler = optuna.samplers.TPESampler(seed=sampler_seed)\\n\",\n",
      "    \"    study = optuna.create_study(direction=direction,\\n\",\n",
      "    \"                                pruner=optuna.pruners.HyperbandPruner(),\\n\",\n",
      "    \"                                sampler=sampler\\n\",\n",
      "    \"                                )\\n\",\n",
      "    \"# save others\\n\",\n",
      "    \"optuna_setup_dict['devid_suffix'] = devid_suffix\\n\",\n",
      "    \"optuna_setup_dict['study_log_input'] = _study_log\\n\",\n",
      "    \"optuna_setup_dict['study_log_output']  = 'study_log{}'.format(len(glob.glob(study_dir+\\\"/study_log*\\\"))+1)\\n\",\n",
      "    \"optuna_setup_dict['_test_mode'] = str(_test_mode)\\n\",\n",
      "    \"\\n\",\n",
      "    \"with open(__file__,\\\"r\\\") as input:\\n\",\n",
      "    \"    print(input.read())\\n\",\n",
      "    \"search(optuna_setup_dict,study)\\n\"\n",
      "   ]\n",
      "  }\n",
      " ],\n",
      " \"metadata\": {\n",
      "  \"kernelspec\": {\n",
      "   \"display_name\": \"_DEMO\",\n",
      "   \"language\": \"python\",\n",
      "   \"name\": \"python3\"\n",
      "  },\n",
      "  \"language_info\": {\n",
      "   \"codemirror_mode\": {\n",
      "    \"name\": \"ipython\",\n",
      "    \"version\": 3\n",
      "   },\n",
      "   \"file_extension\": \".py\",\n",
      "   \"mimetype\": \"text/x-python\",\n",
      "   \"name\": \"python\",\n",
      "   \"nbconvert_exporter\": \"python\",\n",
      "   \"pygments_lexer\": \"ipython3\",\n",
      "   \"version\": \"3.9.7\"\n",
      "  },\n",
      "  \"orig_nbformat\": 4,\n",
      "  \"vscode\": {\n",
      "   \"interpreter\": {\n",
      "    \"hash\": \"8f1124459c8ba36914f5d8bdd75c23dfe4aa23710f8c67711f4e178dcf138635\"\n",
      "   }\n",
      "  }\n",
      " },\n",
      " \"nbformat\": 4,\n",
      " \"nbformat_minor\": 2\n",
      "}\n",
      "\n",
      "Sampler used is TPESampler\n",
      "Objective ...\n",
      "setting 1 to attr study_total_n_trials \n",
      "setting 1 to attr study_num_per_batch \n",
      "setting minimize to attr study_direction \n",
      "setting MAE to attr loss_mode \n",
      "setting default to attr eval_method \n",
      "setting GNN to attr modeltype_pyfilename \n",
      "setting formula_GPU to attr opttrain_mode \n",
      "setting _graphBiEDGE2DATA(LIG).csv to attr dataset_edge(LIG)_filename \n",
      "setting _graphNODEDATA(LIG).csv to attr dataset_node(LIG)_filename \n",
      "setting _rawDATA(RCT).xlsx to attr dataset(RCT)_filename \n",
      "setting _rawDATA(RCTxLIG)_deltaE.xlsx to attr dataset(RCTxLIG)_filename \n",
      "setting 1 to attr constant_node_num \n",
      "setting ['boolean'] to attr input_ndata_list \n",
      "setting ['weight'] to attr input_edata_list \n",
      "setting 0.2 to attr test_split \n",
      "setting 0.2 to attr vali_split \n",
      "setting {'categorical': [5.0, 6.0, 7.0]} to param stage1_cutoff \n",
      "setting 1e-05 to attr stage2_converge \n",
      "setting 0.8 to attr stage2_fluctuate_pc \n",
      "setting 0.2 to attr stage2_fluctuate_dv \n",
      "setting Adam to attr optimizer_chosen \n",
      "setting {'categorical': [0.0005, 0.001, 0.002, 0.005]} to param Adam|learning_rate \n",
      "setting 100 to attr max_epoch_num \n",
      "setting 10 to attr batch_size \n",
      "setting {'categorical': [2, 3, 4]} to param model|MPNN|message_dim \n",
      "setting 1 to attr model|MPNN|hidden_dim \n",
      "setting LeakyReLU to attr model|MPNN|activation \n",
      "setting 0.01 to attr model|MPNN|activation_alpha \n",
      "setting 0 to attr model|MPNN|use_bias \n",
      "setting 1 to attr model|MPNN|include_edge_feat \n",
      "setting {'categorical': [3, 4, 5]} to param model|MPNN|layer_num \n",
      "setting 1 to attr model|MPNN|repeat_msgANDhidden_layer \n",
      "setting 1 to attr model|dense_E1|units \n",
      "setting sigmoid to attr model|dense_E1|activation \n",
      "setting 0 to attr model|dense_E1|use_bias \n",
      "setting l2 to attr model|dense_E1|kernel_regularizer \n",
      "setting None to attr model|dense_E1|kernel_constraint \n",
      "setting 1 to attr model|dense_E2|units \n",
      "setting sigmoid to attr model|dense_E2|activation \n",
      "setting 0 to attr model|dense_E2|use_bias \n",
      "setting l2 to attr model|dense_E2|kernel_regularizer \n",
      "setting NonNeg to attr model|dense_E2|kernel_constraint \n",
      "setting 1 to attr model|dense_output|units \n",
      "setting linear to attr model|dense_output|activation \n",
      "setting 0 to attr model|dense_output|use_bias \n",
      "setting None to attr model|dense_output|kernel_regularizer \n",
      "setting NonNeg to attr model|dense_output|kernel_constraint \n",
      "setting Optuna_StudyResult_DEMO_11 to attr study_foldername \n",
      "setting 8439 to attr sampler_seed \n",
      "setting  to attr devid_suffix \n",
      "setting None to attr study_log_input \n",
      "setting study_log1 to attr study_log_output \n",
      "setting None to attr _test_mode \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# get current filelog\n",
    "if _devid:\n",
    "    devid_suffix = \"_#{}\".format(_devid)\n",
    "else:\n",
    "    devid_suffix = \"\"\n",
    "\n",
    "if _cont_study:\n",
    "    print(\"...continuing Study\")\n",
    "    # get continue study file & setup filename\n",
    "    study_foldername = _cont_study\n",
    "    study_dir = os.path.join(currentdir,study_foldername)\n",
    "    if _test_mode != \"1\":\n",
    "        with open(os.path.join(study_dir,\"_setup.inp\"), \"r\") as input:\n",
    "            optuna_setup_dict = json.loads(str(input.read()))\n",
    "\n",
    "    # save folder name\n",
    "    print(optuna_setup_dict)\n",
    "    optuna_setup_dict[\"study_foldername\"] = study_foldername\n",
    "    # load study\n",
    "    with open(os.path.join(study_dir,\"study\"), 'rb') as _study_input:\n",
    "        study = pickle.load(_study_input)\n",
    "\n",
    "elif _new_filenames:\n",
    "    print(\"...creating new Study\")\n",
    "    # get setup filename\n",
    "    setup_filename = _new_filenames[0].replace(\".inp\",\"\")\n",
    "    with open(setup_filename+\".inp\", \"r\") as input:\n",
    "        optuna_setup_dict = json.loads(str(input.read()))\n",
    "\n",
    "    # create study folder\n",
    "    all_begin = time.time()\n",
    "    z = datetime.datetime.now()\n",
    "    if _DEMO:\n",
    "        study_foldername = \"Optuna_StudyResult_DEMO_{0:06d}{1}\".format(z.microsecond,devid_suffix).replace(\"-\",\"\")\n",
    "    else:\n",
    "        study_foldername = \"Optuna_StudyResult_{0}_{1:02d}{2:02d}{3}\".format(z.date(),z.hour,z.minute,devid_suffix).replace(\"-\",\"\")\n",
    "    optuna_setup_dict[\"study_foldername\"] = study_foldername\n",
    "    study_dir = currentdir+\"/\"+study_foldername\n",
    "    if _test_mode != \"1\":\n",
    "        if not( os.path.exists(study_dir) ):\n",
    "            os.makedirs(study_dir)\n",
    "        with open(os.path.join(currentdir,study_foldername,\"_setup.inp\"), \"w+\") as output:\n",
    "            output.write(json.dumps(optuna_setup_dict,indent=4))\n",
    "\n",
    "    # gen seed for sampler\n",
    "    if optuna_setup_dict.get('sampler_seed') == None:\n",
    "        sampler_seed = random.randint(0,9999)\n",
    "        optuna_setup_dict['sampler_seed'] = sampler_seed\n",
    "    sampler_seed = optuna_setup_dict['sampler_seed']\n",
    "\n",
    "    # create study\n",
    "    direction = optuna_setup_dict['study_direction']\n",
    "    sampler = optuna.samplers.TPESampler(seed=sampler_seed)\n",
    "    study = optuna.create_study(direction=direction,\n",
    "                                pruner=optuna.pruners.HyperbandPruner(),\n",
    "                                sampler=sampler\n",
    "                                )\n",
    "# save others\n",
    "optuna_setup_dict['devid_suffix'] = devid_suffix\n",
    "optuna_setup_dict['study_log_input'] = _study_log\n",
    "optuna_setup_dict['study_log_output']  = 'study_log{}'.format(len(glob.glob(study_dir+\"/study_log*\"))+1)\n",
    "optuna_setup_dict['_test_mode'] = str(_test_mode)\n",
    "\n",
    "with open(__file__,\"r\") as input:\n",
    "    print(input.read())\n",
    "search(optuna_setup_dict,study)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_DEMO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f1124459c8ba36914f5d8bdd75c23dfe4aa23710f8c67711f4e178dcf138635"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
